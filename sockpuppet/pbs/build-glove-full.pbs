#!/usr/bin/env bash
#PBS -k eo
#PBS -l nodes=1:ppn=28,walltime=168:00:00
#PBS -q extended

SOCKPUPPET="$HOME/code/Sock-Puppet/sockpuppet"
TOKENIZE="$SOCKPUPPET/model/dataset/twitter_tokenize.py"
TWITTER_DIR="$HOME/scratch/twitter"
WORKING_DIR="$HOME/scratch/twitter"
CUSTOM_GLOVE_DIR="$TWITTER_DIR/custom-glove"
TOKENIZED_DIR="$CUSTOM_GLOVE_DIR/tokenized"

MEMORY=100

mkdir -p "$TOKENIZED_DIR"
cd "$TWITTER_DIR"

module load shared
module load anaconda/.anaconda

function tokenize() {
    ARCHIVE="$1"
    BASENAME="$(basename -s.7z "$ARCHIVE")"


    echo -e "\t\t$(date) Tokenizing the tweets in $ARCHIVE"

    7z e -so "$ARCHIVE" | \
        jq --raw-input --raw-output --from-file "$SOCKPUPPET/jq/get-text.jq" | \
        python3 -OO "$TOKENIZE" \
    > "$TOKENIZED_DIR/$BASENAME.txt"

    echo "$\t\t(date) Done tokenizing $ARCHIVE"
}

echo -e "$(date) Start building custom GloVe model"

###############################################################################
## Building the corpus 
###############################################################################

echo -e "\t$(date) Start building corpus"

while read i; do
    tokenize "$i" &
done < <(find . -name '*twitter-stream-*.7z' -or -name 'tweets.*.7z')

wait

echo -e "\t\t$(date) Combining tokenized corpii"

cat $TOKENIZED_DIR/*.txt > "$CUSTOM_GLOVE_DIR/corpus.txt"
rm -rf "$TOKENIZED_DIR"

echo -e "\t$(date) Done building corpus, available in $CUSTOM_GLOVE_DIR/corpus.txt"

#------------------------------------------------------------------------------


###############################################################################
## Counting vocabulary occurrences
###############################################################################
MAX_VOCAB=3000000

echo -e "\t$(date) Counting word appearences in corpus"
echo -e "\t\t-max-vocab = $MAX_VOCAB words"

vocab_count \
    -verbose 2 \
    -max-vocab "$MAX_VOCAB" \
    < "$CUSTOM_GLOVE_DIR/corpus.txt" \
    > "$CUSTOM_GLOVE_DIR/vocab.txt"

echo -e "\t$(date) Done counting words, results available in $CUSTOM_GLOVE_DIR/vocab.txt"

#------------------------------------------------------------------------------


###############################################################################
## Building coooccurrence matrix
###############################################################################

echo -e "\t$(date) Building cooccurrence matrix"
echo -e "\t\t-memory = $MEMORY GB"

OVERFLOW_DIR="$CUSTOM_GLOVE_DIR/overflow"
mkdir -p "$OVERFLOW_DIR"

cooccur \
    -verbose 3 \
    -memory "$MEMORY" \
    -vocab-file "$CUSTOM_GLOVE_DIR/vocab.txt" \
    -overflow-file "$OVERFLOW_DIR/overflow" \
    < "$CUSTOM_GLOVE_DIR/corpus.txt" \
    > "$CUSTOM_GLOVE_DIR/cooccurrences.bin"

rm -rf "$OVERFLOW_DIR"
echo -e "\t$(date) Done building cooccurrence matrix, results available in $CUSTOM_GLOVE_DIR/cooccurrences.bin"

#------------------------------------------------------------------------------

###############################################################################
## Shuffling coooccurrence matrix
###############################################################################

echo -e "\t$(date) Shuffling cooccurrence matrix"
echo -e "\t\t-memory = $MEMORY GB"

TEMP_SHUFFLE_DIR="$CUSTOM_GLOVE_DIR/temp_shuffle"
mkdir -p "$TEMP_SHUFFLE_DIR"

shuffle \
    -memory "$MEMORY" \
    -temp-file "$TEMP_SHUFFLE_DIR/temp_shuffle" \
    < "$CUSTOM_GLOVE_DIR/cooccurrences.bin" \
    > "$CUSTOM_GLOVE_DIR/cooccurrences.shuf.bin"

rm -rf "$TEMP_SHUFFLE_DIR"
echo -e "\t$(date) Done shuffling cooccurrence matrix, available in $CUSTOM_GLOVE_DIR/cooccurrences.shuf.bin"

#------------------------------------------------------------------------------

###############################################################################
## Training the GloVe model
###############################################################################

VECTOR_SIZE=100
ITER=25
THREADS="$(nproc)"

echo -e "\t$(date) Training GloVe model"
echo -e "\t\t-vector-size = $VECTOR_SIZE"
echo -e "\t\t-threads = $THREADS"
echo -e "\t\t-iter = $ITER"

glove \
    -input-file "$CUSTOM_GLOVE_DIR/cooccurrences.shuf.bin" \
    -vocab-file "$CUSTOM_GLOVE_DIR/vocab.txt" \
    -save-file "$CUSTOM_GLOVE_DIR/glove" \
    -gradsq-file "$CUSTOM_GLOVE_DIR/gradsq" \
    -vector-size "$VECTOR_SIZE" \
    -iter "$ITER" \
    -threads "$THREADS"

echo -e "\t$(date) Done training GloVe model, available in $CUSTOM_GLOVE_DIR/glove"

#------------------------------------------------------------------------------

###############################################################################
## Post-processing the GloVe model
###############################################################################

echo -e "\t$(date) Prepending <pad> and <unknown> to GloVe model"

python3 -c "print('<pad>', *([0.0] * $VECTOR_SIZE))" >> "$CUSTOM_GLOVE_DIR/glove.txt~"
python3 -c "print('<unknown>', *([0.0] * $VECTOR_SIZE))" >> "$CUSTOM_GLOVE_DIR/glove.txt~"
cat "$CUSTOM_GLOVE_DIR/glove.txt" >> "$CUSTOM_GLOVE_DIR/glove.txt~"

mv "$CUSTOM_GLOVE_DIR/glove.txt~" "$CUSTOM_GLOVE_DIR/glove.txt"

echo -e "\t$(date) Compressing GloVe model"

xz -9 --extreme --stdout --threads=0 -vv "$CUSTOM_GLOVE_DIR/glove.txt" > "$CUSTOM_GLOVE_DIR/glove.xz"

echo -e "\t$(date) Done compressing GloVe model, available in $CUSTOM_GLOVE_DIR/glove.xz"

#------------------------------------------------------------------------------

echo "$(date) Done building custom GloVe model"
