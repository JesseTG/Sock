#!/usr/bin/env bash
#PBS -k eo
#PBS -l nodes=1:ppn=28,walltime=168:00:00
#PBS -q extended

module load anaconda/.anaconda

TOKENIZE="$HOME/code/Sock-Puppet/sockpuppet/model/dataset/twitter_tokenize.py"
TWITTER_DIR="$HOME/scratch/twitter"
TOKENIZED_DIR="$TWITTER_DIR/tokenized"

mkdir -p "$TOKENIZED_DIR"
cd "$TWITTER_DIR"

function tokenize() {
    ARCHIVE="$1"
    OUTDIR="$2"
    BASENAME="$(basename -s.7z "$ARCHIVE")"


    echo "$(date) Tokenizing the tweets in $ARCHIVE"

    7z e -so "$ARCHIVE" | \
        jq -rR 'try (fromjson | .text, .full_text | values) catch empty' | \
        python3 -OO "$TOKENIZE" \
    > "$TOKENIZED_DIR/$BASENAME.txt"

    # TODO: Output newlines as literal "\n"
    echo "$(date) Done processing $ARCHIVE"
}


while read i; do
    tokenize "$i" "$TOKENIZED_DIR" &
done < <(find . -name '*twitter-stream-*.7z' -or -name 'tweets.*.7z')

wait

cat $TOKENIZED_DIR/*.txt > "$TWITTER_DIR/corpus.txt"
rm -rf "$TOKENIZED_DIR"

echo "$(date) All done"
